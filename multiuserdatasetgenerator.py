# -*- coding: utf-8 -*-
"""MultiUserDatasetGenerator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DHPN9-GnCMc6ve66PT0W7LdnVlmJZpwb
"""

import numpy as np
import json
import os
from tqdm import tqdm
from typing import Dict, List, Tuple, Any
import copy
from config import FRAMEWORK_CONFIG

# Simulation Settings
# Use FRAMEWORK_CONFIG sim settings as the single source of truth
SIM_SETTINGS = {
    **FRAMEWORK_CONFIG.get("sim_settings", {}),
    "base_station_power_dBm_range": FRAMEWORK_CONFIG.get("power_range_dB", [20,45]),
    "default_bs_power_dBm": FRAMEWORK_CONFIG.get("default_bs_power_dBm", 20),
    # Preserve local defaults if not specified in config
    "iterations": 1000,
    "monte_carlo_runs": 500,
}

# Application definitions with base SNR requirements
# APPLICATIONS dictionary moved to config.py under FRAMEWORK_CONFIG["snr_calculation"]["applications"]

"""#**Algorithms**"""

import numpy as np
from typing import Dict, List, Tuple, Any, Sequence, Optional
import copy
import json
import os
import glob

def get_latest_agentic_power(scenario_name=None):
    """
    Get the final power from the most recent agentic system run.
    
    Args:
        scenario_name: Optional scenario name to filter results for
        
    Returns:
        float: Final power in dBm from the latest agentic run, or None if not found
    """
    results_dir = "results"
    if not os.path.exists(results_dir):
        return None
    
    # Get all result files sorted by timestamp (newest first)
    result_files = glob.glob(os.path.join(results_dir, "session_results_*.json"))
    if not result_files:
        return None
    
    result_files.sort(reverse=True)  # Newest first
    
    for result_file in result_files:
        try:
            with open(result_file, 'r') as f:
                data = json.load(f)
            
            # If scenario_name is specified, filter by scenario
            if scenario_name is None or data.get("scenario_name", "") == scenario_name:
                final_power = data.get("final_power_dBm")
                if final_power is not None:
                    print(f"🔋 Found agentic system final power: {final_power:.1f} dBm from {os.path.basename(result_file)}")
                    return float(final_power)
        except (json.JSONDecodeError, KeyError, ValueError):
            continue
    
    return None

class RISAlgorithms:
    """RIS algorithms adapted for joint multi-user sum-rate optimization."""

    def __init__(self, noise_floor: float = 1e-12):
        self.noise_floor = noise_floor

    def compute_cascaded_channel(self, H: np.ndarray, h_1: np.ndarray) -> np.ndarray:

      h_1 = np.asarray(h_1).reshape(-1)           # (N,)
      H = np.asarray(H)

      # If H is 1D length N -> elementwise multiply
      if H.ndim == 1 and H.shape[0] == h_1.shape[0]:
          h_r = h_1 * H

      # If H is (N, Mbs) -> collapse Mbs dimension (sum) then elementwise multiply
      elif H.ndim == 2 and H.shape[0] == h_1.shape[0]:
          # per-RIS-element aggregate from BS side
          bs_agg = np.sum(H, axis=1)   # (N,)
          h_r = h_1 * bs_agg

      # If H is (Mbs, N) -> treat as transpose
      elif H.ndim == 2 and H.shape[1] == h_1.shape[0]:
          bs_agg = np.sum(H, axis=0)   # (N,)
          h_r = h_1 * bs_agg

      else:
          raise ValueError(f"compute_cascaded_channel: unexpected shapes H={H.shape}, h_1={h_1.shape}")

      return h_r.ravel()

    def compute_effective_channel(self, h_d, h_r, e):
        """
        Compute the effective channel (direct + RIS).
        h_d: direct BS→user channel (scalar)
        h_r: cascaded channel for this user (Nr,)
        e: RIS phase vector (M,) or (M,1)
        """
        # Ensure h_d is a scalar
        h_d_scalar = np.asarray(h_d).flatten()[0] if np.asarray(h_d).size > 0 else 0.0

        # Ensure h_r and e are 1D for dot product
        h_r = np.asarray(h_r).flatten()
        e = np.asarray(e).flatten()

        # Effective channel: h_d + h_rᵀ e
        return h_d_scalar + np.dot(h_r, e)

    def compute_received_power(self, h_d, h_r, e, transmit_power):
        """
        Compute received power for one user.
        h_d: direct BS→user channel (scalar or (Nr,))
        h_r: cascaded channel for this user (Nr,)
        e: RIS phase vector (M,) or (M,1)
        """
        # Effective channel: h_d + h_rᵀ e
        combined = self.compute_effective_channel(h_d, h_r, e)
        return transmit_power * np.abs(combined) ** 2

    def compute_snr(self, power, noise_power=1e-12):
        """Compute SNR in dB for a given power."""
        return 10 * np.log10(power / noise_power)

    def compute_sum_rate(self, h_d_list: Sequence[np.ndarray], H: np.ndarray,
                         h_1_list: Sequence[np.ndarray], e: np.ndarray, transmit_power: float,
                         noise_power: Optional[float] = None,
                         power_allocation: Optional[Sequence[float]] = None) -> float:
        """Compute sum rate across users."""
        if noise_power is None:
            noise_power = self.noise_floor

        K = len(h_d_list)
        if power_allocation is None:
            Pk = np.full(K, transmit_power / max(1, K))
        else:
            Pk = np.asarray(power_allocation, dtype=float)

        sum_rate = 0.0
        for k in range(K):
            h_eff_k = self.compute_effective_channel(h_d_list[k], self.compute_cascaded_channel(H, h_1_list[k]), e)
            received_power_k = Pk[k] * np.abs(h_eff_k) ** 2
            snr_k = received_power_k / noise_power
            sum_rate += np.log2(1.0 + snr_k)

        return sum_rate

    def compute_per_user_snrs(self, h_d_list: Sequence[np.ndarray], H: np.ndarray,
                              h_1_list: Sequence[np.ndarray], e: np.ndarray, transmit_power: float,
                              noise_power: Optional[float] = None,
                              power_allocation: Optional[Sequence[float]] = None) -> np.ndarray:
        """Compute per-user SNRs in dB."""
        if noise_power is None:
            noise_power = self.noise_floor

        K = len(h_d_list)
        if power_allocation is None:
            Pk = np.full(K, transmit_power / max(1, K))
        else:
            Pk = np.asarray(power_allocation, dtype=float)

        per_user_snrs_linear = np.zeros(K)
        for k in range(K):
            h_eff_k = self.compute_effective_channel(h_d_list[k], self.compute_cascaded_channel(H, h_1_list[k]), e)
            received_power_k = Pk[k] * np.abs(h_eff_k) ** 2
            per_user_snrs_linear[k] = received_power_k / noise_power

        return 10 * np.log10(per_user_snrs_linear + 1e-12) # Add small epsilon for log stability

    # -------------------------
    # Gradient for GD (θ-space)
    # -------------------------
    def compute_gd_gradient_multi(self, h_d_list: Sequence[np.ndarray], H: np.ndarray,
                                  h_1_list: Sequence[np.ndarray], theta: np.ndarray,
                                  transmit_power: float, noise_power: Optional[float] = None,
                                  power_allocation: Optional[Sequence[float]] = None) -> np.ndarray:
        """
        Compute gradient of sum-rate w.r.t. phases theta (real vector, length N).
        dR/dθ_n = sum_k [ (1/ln2) * (1/(1+snr_k)) * (Pk/noise) * d|h_eff_k|^2/dθ_n ]
        where d|h_eff|^2/dθ_n = 2 * Re{ j * h_rk[n] * e_n * h_eff_k.conj() }.
        Returns real gradient array length N.
        """
        if noise_power is None:
            noise_power = self.noise_floor
        K = len(h_d_list)
        N = theta.size
        e = np.exp(1j * theta)
        # power allocation
        if power_allocation is None:
            Pk = np.full(K, transmit_power / max(1, K))
        else:
            Pk = np.asarray(power_allocation, dtype=float)

        grad_theta = np.zeros(N, dtype=float)
        # compute per-user quantities
        h_r_list = [self.compute_cascaded_channel(H, h1) for h1 in h_1_list]
        h_eff = np.zeros(K, dtype=complex)
        snrs = np.zeros(K, dtype=float)
        for k in range(K):
            h_eff[k] = self.compute_effective_channel(h_d_list[k], h_r_list[k], e)
            snrs[k] = (np.abs(h_eff[k]) ** 2) * Pk[k] / noise_power

        const = 1.0 / np.log(2.0)
        for n in range(N):
            # derivative of |h_eff_k|^2 wrt theta_n across users
            # d|h_eff|^2/dθ_n = 2 * Re{ j * h_rk[n] * e_n * h_eff_k.conj() }
            e_n = e[n]
            accum = 0.0
            for k in range(K):
                h_rkn = h_r_list[k][n]
                # scalar derivative
                d_abs2_dth = 2.0 * np.real(1j * h_rkn * e_n * np.conj(h_eff[k]))
                factor = (Pk[k] / noise_power) * (1.0 / (1.0 + snrs[k]))  # from d log2(1+snr)/d|h|^2
                accum += factor * d_abs2_dth
            grad_theta[n] = const * accum
        return grad_theta  # real vector

    # -------------------------
    # Algorithm: Adam-GD on theta
    # -------------------------
    def gradient_descent_adam_multi(self, h_d_list: Sequence[np.ndarray], H: np.ndarray,
                                    h_1_list: Sequence[np.ndarray], N: int, transmit_power: float,
                                    learning_rate: float = 0.05, max_iterations: int = 1000,
                                    beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8,
                                    discrete_phases: Optional[Sequence[float]] = None,
                                    noise_power: Optional[float] = None,
                                    power_allocation: Optional[Sequence[float]] = None) -> Tuple[np.ndarray, List[float], List[np.ndarray], List[int]]:
        """Adam optimizer applied directly to phase vector theta (real)."""
        if noise_power is None:
            noise_power = self.noise_floor
        theta = np.random.uniform(0, 2 * np.pi, N)
        e = np.exp(1j * theta)

        sum_rates = [self.compute_sum_rate(h_d_list, H, h_1_list, e, transmit_power,
                                           noise_power=noise_power, power_allocation=power_allocation)]
        per_user_snrs = [self.compute_per_user_snrs(h_d_list, H, h_1_list, e, transmit_power,
                                                    noise_power=noise_power, power_allocation=power_allocation)]
        iterations = [0]

        m = np.zeros(N, dtype=float)
        v = np.zeros(N, dtype=float)
        t = 0

        for iteration in range(1, max_iterations + 1):
            t += 1
            grad = self.compute_gd_gradient_multi(h_d_list, H, h_1_list, theta,
                                                  transmit_power, noise_power, power_allocation)
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * (grad ** 2)
            m_hat = m / (1 - beta1 ** t)
            v_hat = v / (1 - beta2 ** t)
            theta = theta + learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))  # ascend
            # optional discrete quantization
            if discrete_phases is not None:
                theta = np.array([min(discrete_phases, key=lambda ph: abs(ph - th)) for th in theta])
            # canonicalize
            theta = np.mod(theta, 2 * np.pi)
            e = np.exp(1j * theta)
            sum_rates.append(self.compute_sum_rate(h_d_list, H, h_1_list, e, transmit_power,
                                                  noise_power=noise_power, power_allocation=power_allocation))
            per_user_snrs.append(self.compute_per_user_snrs(h_d_list, H, h_1_list, e, transmit_power,
                                                            noise_power=noise_power, power_allocation=power_allocation))
            iterations.append(iteration)
        return theta, sum_rates, per_user_snrs, iterations

    # -------------------------
    # Manifold-style Adam (θ-space Adam + retraction)
    # -------------------------
    def manifold_optimization_adam_multi(self, h_d_list: Sequence[np.ndarray], H: np.ndarray,
                                         h_1_list: Sequence[np.ndarray], N: int, transmit_power: float,
                                         learning_rate: float = 0.05, max_iterations: int = 1000,
                                         beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8,
                                         discrete_phases: Optional[Sequence[float]] = None,
                                         noise_power: Optional[float] = None,
                                         power_allocation: Optional[Sequence[float]] = None) -> Tuple[np.ndarray, List[float], List[np.ndarray], List[int]]:
        """
        A practical manifold approach: we optimize theta (real) using Adam, then re-project
        to the unit-modulus complex vector e = exp(j theta) (this is the retraction).
        """
        if noise_power is None:
            noise_power = self.noise_floor
        theta = np.random.uniform(0, 2 * np.pi, N)
        e = np.exp(1j * theta)

        sum_rates = [self.compute_sum_rate(h_d_list, H, h_1_list, e, transmit_power,
                                           noise_power=noise_power, power_allocation=power_allocation)]
        per_user_snrs = [self.compute_per_user_snrs(h_d_list, H, h_1_list, e, transmit_power,
                                                    noise_power=noise_power, power_allocation=power_allocation)]
        iterations = [0]

        m = np.zeros(N, dtype=float)
        v = np.zeros(N, dtype=float)
        t = 0

        for iteration in range(1, max_iterations + 1):
            t += 1
            grad_theta = self.compute_gd_gradient_multi(h_d_list, H, h_1_list, theta,
                                                        transmit_power, noise_power, power_allocation)
            # Riemannian-style Adam on theta (real)
            m = beta1 * m + (1 - beta1) * grad_theta
            v = beta2 * v + (1 - beta2) * (grad_theta ** 2)
            m_hat = m / (1 - beta1 ** t)
            v_hat = v / (1 - beta2 ** t)
            theta = theta + learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))  # ascend
            # quantize if needed
            if discrete_phases is not None:
                theta = np.array([min(discrete_phases, key=lambda ph: abs(ph - th)) for th in theta])
            theta = np.mod(theta, 2 * np.pi)
            e = np.exp(1j * theta)  # retraction to manifold
            sum_rates.append(self.compute_sum_rate(h_d_list, H, h_1_list, e, transmit_power,
                                                  noise_power=noise_power, power_allocation=power_allocation))
            per_user_snrs.append(self.compute_per_user_snrs(h_d_list, H, h_1_list, e, transmit_power,
                                                            noise_power=noise_power, power_allocation=power_allocation))
            iterations.append(iteration)
        return theta, sum_rates, per_user_snrs, iterations

    # -------------------------
    # Alternating Optimization (AO) for sum-rate
    # -------------------------
    def alternating_optimization_multi(self, h_d_list: Sequence[np.ndarray], H: np.ndarray,
                                       h_1_list: Sequence[np.ndarray], N: int, transmit_power: float,
                                       max_iterations: int = 200, tolerance: float = 1e-6,
                                       discrete_phases: Optional[Sequence[float]] = None,
                                       noise_power: Optional[float] = None,
                                       power_allocation: Optional[Sequence[float]] = None,
                                       grid_search_points: int = 32) -> Tuple[np.ndarray, List[float], List[np.ndarray], List[int]]:
        """
        AO that updates each RIS element sequentially by maximizing the current sum-rate
        w.r.t that single element's phase. For robustness we use short grid-search over candidate angles.
        """
        if noise_power is None:
            noise_power = self.noise_floor
        theta = np.random.uniform(0, 2 * np.pi, N)
        e = np.exp(1j * theta)

        sum_rates = [self.compute_sum_rate(h_d_list, H, h_1_list, e, transmit_power,
                                           noise_power=noise_power, power_allocation=power_allocation)]
        per_user_snrs = [self.compute_per_user_snrs(h_d_list, H, h_1_list, e, transmit_power,
                                                    noise_power=noise_power, power_allocation=power_allocation)]
        iterations = [0]

        angle_grid = np.linspace(0, 2*np.pi, grid_search_points, endpoint=False)

        for iteration in range(1, max_iterations + 1):
            prev_rate = sum_rates[-1]
            # update each element
            for n in range(N):
                best_angle = theta[n]
                best_rate = -np.inf
                # precompute e without element n contribution
                e_without_n = e.copy()
                e_without_n[n] = 1.0  # remove contribution, we'll multiply candidate e_n
                # try candidates
                for a in angle_grid:
                    candidate_e = e.copy()
                    candidate_e[n] = np.exp(1j * a)
                    # evaluate sum-rate quickly
                    rate_candidate = self.compute_sum_rate(h_d_list, H, h_1_list, candidate_e, transmit_power,
                                                           noise_power=noise_power, power_allocation=power_allocation)
                    if rate_candidate > best_rate:
                        best_rate = rate_candidate
                        best_angle = a
                theta[n] = best_angle
                e[n] = np.exp(1j * best_angle)
                # optional discrete quantization after whole-element update (or inside if desired)
            if discrete_phases is not None:
                theta = np.array([min(discrete_phases, key=lambda ph: abs(ph - th)) for th in theta])
                theta = np.mod(theta, 2*np.pi)
                e = np.exp(1j * theta)

            current_rate = self.compute_sum_rate(h_d_list, H, h_1_list, e, transmit_power,
                                                 noise_power=noise_power, power_allocation=power_allocation)
            sum_rates.append(current_rate)
            per_user_snrs.append(self.compute_per_user_snrs(h_d_list, H, h_1_list, e, transmit_power,
                                                            noise_power=noise_power, power_allocation=power_allocation))
            iterations.append(iteration)

            if abs(current_rate - prev_rate) < tolerance * max(1.0, abs(prev_rate)):
                break

        return theta, sum_rates, per_user_snrs, iterations

"""#**Algorithm Check**"""

import numpy as np
import matplotlib.pyplot as plt

def generate_channels(K=5, N=1, Mbs=1, seed=42, sim_settings=None):
    np.random.seed(seed)

    if sim_settings is None:
        # Use default settings if none provided
        sim_settings = {
            "ris_coord": (50, 0, 10),
            "bs_coord": (0, 0, 10) # Added BS coord for potential future use
        }

    # Random user positions in [0, 100] m square
    user_positions = np.random.rand(K, 2) * 100

    # Direct BS->user channels (assume Rayleigh fading + pathloss)
    h_d_list = []
    for pos in user_positions:
        dist = np.linalg.norm(pos - np.array(sim_settings["bs_coord"])[:2]) + 1e-3 # Use BS coord from settings
        pathloss = dist**-2.0
        h_d_list.append((np.random.randn() + 1j*np.random.randn()) * np.sqrt(pathloss))

    # RIS location from settings
    ris_pos = np.array(sim_settings["ris_coord"])[:2] # Use RIS coord from settings

    H = (np.random.randn(N, Mbs) + 1j*np.random.randn(N, Mbs)) / np.sqrt(2)  # BS->RIS
    h_1_list = []
    for pos in user_positions:
        dist = np.linalg.norm(ris_pos - pos) + 1e-3
        pathloss = dist**-2.0
        h_1_list.append((np.random.randn(N) + 1j*np.random.randn(N)) * np.sqrt(pathloss))

    return h_d_list, H, h_1_list, user_positions

def run_test(scenario_name="5U_B", override_power_dBm=None):
    # Ensure plots directory exists
    os.makedirs("plots", exist_ok=True)
    
    # Load a sample scenario to get the number of users dynamically
    from scenario import CASES
    sample_scenario = CASES.get(scenario_name, CASES["5U_B"])  # Fallback to 5U_B
    
    K = sample_scenario["num_users"]  # Get number of users from scenario
    N = SIM_SETTINGS["ris_elements"]  # Use configured RIS elements
    
    # Use power values - prioritize override_power_dBm if provided, otherwise use config
    bs_power_dBm = override_power_dBm if override_power_dBm is not None else SIM_SETTINGS["default_bs_power_dBm"]
    noise_power_dBm = SIM_SETTINGS["noise_power_dBm"]
    
    # Convert to linear watts
    tx_power = 10 ** ((bs_power_dBm - 30) / 10)  # dBm to watts
    noise_power_linear = 10 ** ((noise_power_dBm - 30) / 10)  # dBm to watts
    
    algo = RISAlgorithms(noise_floor=noise_power_linear)
    
    power_source = "agentic system final power" if override_power_dBm is not None else "config default power"
    print(f"Running test with scenario {scenario_name}: {K} users, {N} RIS elements")

    # Pass SIM_SETTINGS to generate_channels
    h_d_list, H, h_1_list, user_positions = generate_channels(K=K, N=N, sim_settings=SIM_SETTINGS)

    # Random RIS (baseline)
    theta_rand = np.random.uniform(0, 2*np.pi, N)
    e_rand = np.exp(1j*theta_rand)
    snrs_rand = algo.compute_per_user_snrs(h_d_list, H, h_1_list, e_rand, tx_power)
    rate_rand = algo.compute_sum_rate(h_d_list, H, h_1_list, e_rand, tx_power)
    print("Random baseline SNRs (dB):", np.round(snrs_rand,2))
    print("Random baseline sum-rate:", rate_rand)

    # Adam-GD
    theta_adam, rates_adam, snrs_adam_hist, _ = algo.gradient_descent_adam_multi(
        h_d_list, H, h_1_list, N, tx_power, max_iterations=500, learning_rate=0.01
    )
    final_snrs_adam = snrs_adam_hist[-1] if snrs_adam_hist else []
    print(f"📊 Adam-GD Final SNRs (dB): {np.round(final_snrs_adam, 2)}")

    # Manifold Adam
    theta_man, rates_man, snrs_man_hist, _ = algo.manifold_optimization_adam_multi(
        h_d_list, H, h_1_list, N, tx_power, max_iterations=500, learning_rate=0.01
    )
    final_snrs_man = snrs_man_hist[-1] if snrs_man_hist else []
    print(f"📊 Manifold Final SNRs (dB): {np.round(final_snrs_man, 2)}")

    # AO
    theta_ao, rates_ao, snrs_ao_hist, _ = algo.alternating_optimization_multi(
        h_d_list, H, h_1_list, N, tx_power, max_iterations=1000
    )
    final_snrs_ao = snrs_ao_hist[-1] if snrs_ao_hist else []
    print(f"📊 Alternating Opt Final SNRs (dB): {np.round(final_snrs_ao, 2)}")

    # Visualization
    fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # Increased figure size and added a subplot

    # Plot 1: System Layout
    axes[0].scatter(SIM_SETTINGS["bs_coord"][0], SIM_SETTINGS["bs_coord"][1], marker="^", color="red", s=120, label="Base Station")
    axes[0].scatter(SIM_SETTINGS["ris_coord"][0], SIM_SETTINGS["ris_coord"][1], marker="s", color="blue", s=120, label="RIS")
    axes[0].scatter(user_positions[:, 0], user_positions[:, 1], marker="o", color="green", s=80, label="Users")
    axes[0].set_xlabel("X [m]")
    axes[0].set_ylabel("Y [m]")
    axes[0].set_title("System Layout")
    axes[0].legend()
    axes[0].grid(True)


    # Plot 2: Sum-rate evolution
    axes[1].plot(rates_adam, label="Adam-GD")
    axes[1].plot(rates_man, label="Manifold Adam")
    axes[1].plot(rates_ao, label="AO")
    axes[1].axhline(rate_rand, color="k", linestyle="--", label="Random baseline")
    axes[1].set_xlabel("Iteration")
    axes[1].set_ylabel("Sum-rate (bps/Hz)")
    axes[1].set_title("Sum-rate evolution")
    axes[1].legend()

    # Plot 3: Per-user SNR comparison (Main Plot)
    users = np.arange(1, K + 1)
    snrs_final = {
        "Random": snrs_rand,
        "Adam-GD": snrs_adam_hist[-1],
        "Manifold Adam": snrs_man_hist[-1],
        "AO": snrs_ao_hist[-1]
    }
    bar_width = 0.2
    for i, (name, snrs) in enumerate(snrs_final.items()):
        axes[2].bar(users + i * bar_width, snrs, width=bar_width, label=name)
    axes[2].set_xticks(users + 1.5 * bar_width)
    axes[2].set_xticklabels([f"User {i}" for i in users])
    axes[2].set_ylabel("SNR (dB)")
    axes[2].set_title("Per-user SNR comparison")
    axes[2].legend()
    axes[2].grid(axis='y', linestyle='--')
    axes[2].set_ylim(50, None) # Set y-axis to start from 50
    plt.tight_layout()
    
    # Save plot to plots folder and show
    plt.savefig("plots/algorithm_comparison_results.png", dpi=300, bbox_inches='tight')
    plt.show()
    plt.close()


def run_test_with_agentic_power(scenario_name="5U_B"):
    """
    Run algorithm comparison using the final power determined by the agentic system.
    Falls back to config power if no agentic results are found.
    """
    print("=" * 60)
    print("🤖 ALGORITHM COMPARISON WITH AGENTIC POWER")
    print("=" * 60)
    
    # Try to get the final power from the agentic system
    agentic_power = get_latest_agentic_power(scenario_name)
    
    if agentic_power is not None:
        print(f"✅ Using agentic system's optimized power: {agentic_power:.1f} dBm")
        run_test(scenario_name, override_power_dBm=agentic_power)
    else:
        print("⚠️  No agentic results found, using default config power")
        run_test(scenario_name)
    
    print("=" * 60)


if __name__ == "__main__":

    run_test()

"""Increasing the number of RIS elements and the number of users causes significant deviation in the algorithm performance

**However**, It is also possible that increased complexity simply results in an algorithm to converge better in one run than at another time

**Verification** with literature needed to showcase the effect of increasing users and increasing RIS elements on the effectiveness of an algorithm

#**Helper Functions**
"""

# =======================
# Helper methods for RIS
# =======================
class RISHelpers:
    @staticmethod
    def compute_cascaded_channel(H, h_1):
        """Compute cascaded channel for one user.
        H: (N, M)  BS → RIS channel
        h_1: (N, 1) RIS → user channel
        Returns: h_r: (M,) effective cascaded channel for this user
        """
        return (h_1.conj().T @ H).flatten()

    def compute_received_power(self, h_d, h_r, e, transmit_power):
        """Compute received power for one user.
        h_d: direct BS→user channel (scalar or (1,))
        h_r: cascaded channel for this user (M,)
        e: RIS phase vector (M,)
        """
        combined = h_d + np.dot(h_r, e)
        return transmit_power * np.abs(combined) ** 2

    def compute_snr(self, power, noise_power=1e-12):
        """Compute SNR in dB for a given power."""
        return 10 * np.log10(power / noise_power)

    # ---------- GRADIENTS ----------
    def compute_gd_gradient(self, h_ds, h_rs, e, transmit_power):
        """Compute gradient across all users for sum-rate optimization.
        h_ds: list of direct channels, each scalar
        h_rs: list of cascaded channels, each (M,)
        e: RIS phase vector (M,)
        """
        N = len(e)
        grad_total = np.zeros(N)
        for h_d, h_r in zip(h_ds, h_rs):
            combined = h_d + np.dot(h_r, e)
            for m in range(N):
                term = h_r[m] * combined.conj()
                grad_total[m] += -2 * transmit_power * np.real(1j * e[m] * term)
        return grad_total

    def compute_manifold_gradient(self, h_ds, h_rs, e, transmit_power):
        """Compute Euclidean gradient for manifold optimization across all users."""
        grad_total = 0
        for h_d, h_r in zip(h_ds, h_rs):
            combined = h_d + np.dot(h_r, e)
            grad_total += -transmit_power * h_r.conj() * combined
        return grad_total

    # ---------- MANIFOLD UTILITIES ----------
    def project_to_tangent_space(self, e, grad):
        """Project gradient to tangent space."""
        gd_f = grad - np.real(np.multiply(e, grad)) * e
        return gd_f, grad

    def retract_to_manifold(self, e, gd_f, ro=0.01, a_l=0.01, c_l=None):
        """Retract back to manifold."""
        if c_l is None:
            c_l = np.zeros_like(e)
        c_l = -gd_f + ro * (c_l - np.real(c_l * e.conj()) * e)
        e_new = e + a_l * c_l
        e_new = e_new / np.abs(e_new)  # normalize to unit circle
        return e_new, c_l

    def ro_calc(self, grad, grad_k):
        """Calculate ro parameter."""
        numerator = np.real(np.vdot(grad, grad - grad_k))
        denominator = np.linalg.norm(grad_k) ** 2 + 1e-12
        return numerator / denominator

def to_serializable(obj):
      """Recursively convert NumPy types to native Python types."""
      if isinstance(obj, dict):
          return {k: to_serializable(v) for k, v in obj.items()}
      elif isinstance(obj, list):
          return [to_serializable(v) for v in obj]
      elif isinstance(obj, np.generic):
          return obj.item()  # Convert NumPy scalars (e.g. np.int64 → int, np.bool_ → bool)
      else:
          return obj

"""#**DataSet Generator Class**"""

class RISDatasetGenerator:

    def __init__(self, sim_settings=None):
        self.sim_settings = sim_settings or SIM_SETTINGS
        self.algorithms = RISAlgorithms()
        self.algorithm_names = ['gradient_descent_adam_multi', 'manifold_optimization_adam_multi', 'alternating_optimization_multi']

    def calculate_required_snr(self, app_type: str, user_coord: Tuple, csi: Dict) -> float:
        """Calculate required SNR based on application, distance, and channel conditions."""
        snr_params = FRAMEWORK_CONFIG.get("snr_calculation", {})
        applications = snr_params.get("applications", {})
        base_snr = applications[app_type]["base_snr_dB"]
        
        # Distance-based margin
        bs_distance = self.calculate_distance_3d(self.sim_settings["bs_coord"], user_coord)
        distance_threshold = snr_params.get("distance_threshold_m", 50)
        distance_penalty_rate = snr_params.get("distance_penalty_db_per_m", 0.05)
        distance_margin = max(0, (bs_distance - distance_threshold) * distance_penalty_rate)

        # Channel condition margins
        channel_margin = 0
        
        # LoS vs NLoS margin
        if not csi["los"]:
            channel_margin += snr_params.get("nlos_penalty_db", 3)

        # Fading margin
        if csi["fading"] == "Rayleigh":
            channel_margin += snr_params.get("rayleigh_fading_penalty_db", 2)
        elif csi["fading"] == "Rician" and "K_factor_dB" in csi:
            k_threshold = snr_params.get("k_factor_threshold_db", 5)
            if csi["K_factor_dB"] < k_threshold:
                channel_margin += snr_params.get("rician_low_k_penalty_db", 1)
        
        # Optional blockage penalty (for future use)
        if snr_params.get("use_blockage_penalty", False) and csi.get("blockage") == "blocked":
            channel_margin += snr_params.get("blockage_penalty_db", 2)

        required_snr = base_snr + distance_margin + channel_margin
        return required_snr

    def calculate_path_loss(self, distance_3d: float) -> float:
        """Calculate path loss in dB using the free space path loss model."""
        return self.sim_settings["PL0_dB"] + 10 * self.sim_settings["gamma"] * np.log10(distance_3d)

    def calculate_distance_3d(self, coord1: Tuple, coord2: Tuple) -> float:
        """Calculate 3D Euclidean distance between two coordinates."""
        return np.sqrt(sum((a - b) ** 2 for a, b in zip(coord1, coord2)))

    def generate_channel(self, user_csi: Dict, distance_3d: float, noise_power_linear: float) -> Tuple:
        """Generate channel coefficients based on CSI information."""
        path_loss_dB = self.calculate_path_loss(distance_3d)
        path_loss_linear = 10 ** (-path_loss_dB / 10)

        if user_csi["fading"] == "Rician":
            K_factor_linear = 10 ** (user_csi["K_factor_dB"] / 10)
            # Generate Rician channel
            los_component = np.sqrt(K_factor_linear / (K_factor_linear + 1))
            nlos_component = np.sqrt(1 / (K_factor_linear + 1)) * (np.random.randn() + 1j * np.random.randn()) / np.sqrt(2)
            channel = (los_component + nlos_component) * np.sqrt(path_loss_linear)
        else:  # Rayleigh
            # Generate Rayleigh channel
            channel = (np.random.randn() + 1j * np.random.randn()) / np.sqrt(2) * np.sqrt(path_loss_linear)

        return channel

    def calculate_received_snr(self, user: Dict, bs_power_linear: float, noise_power_linear: float) -> float:
        """Calculate received SNR for a user without RIS optimization."""
        # Calculate distances
        bs_to_user_dist = self.calculate_distance_3d(self.sim_settings["bs_coord"], user["coord"])
        bs_to_ris_dist = self.calculate_distance_3d(self.sim_settings["bs_coord"], self.sim_settings["ris_coord"])
        ris_to_user_dist = self.calculate_distance_3d(self.sim_settings["ris_coord"], user["coord"])

        # Generate channels
        # Direct channel (BS to User)
        h_d = self.generate_channel(user["csi"], bs_to_user_dist, noise_power_linear)

        # BS to RIS channel
        ris_csi = copy.deepcopy(user["csi"])  # Assume similar propagation characteristics
        H = np.array([self.generate_channel(ris_csi, bs_to_ris_dist, noise_power_linear)
                     for _ in range(self.sim_settings["ris_elements"])]).reshape(1, -1)

        # RIS to User channel
        h_1 = np.array([self.generate_channel(user["csi"], ris_to_user_dist, noise_power_linear)
                       for _ in range(self.sim_settings["ris_elements"])])

        # Calculate received power without RIS optimization (random phases)
        theta = np.random.uniform(0, 2 * np.pi, self.sim_settings["ris_elements"])
        e = np.exp(1j * theta)
        h_r = self.algorithms.compute_cascaded_channel(H, h_1)
        received_power = self.algorithms.compute_received_power(h_d, h_r, e, bs_power_linear)

        # Calculate SNR in dB
        snr_dB = self.algorithms.compute_snr(received_power, noise_power_linear)
        return snr_dB, h_d, H, h_1

    def select_users_for_optimization(self, case_data: Dict) -> Tuple[List[int], List[float]]:
        """Select users that need optimization based on Delta SNR."""
        bs_power_dBm = self.sim_settings["default_bs_power_dBm"]
        bs_power_linear = 10 ** ((bs_power_dBm - 30) / 10)  # Convert dBm to Watts
        noise_power_linear = 10 ** ((self.sim_settings["noise_power_dBm"] - 30) / 10)  # Convert dBm to Watts

        selected_users = []
        delta_snrs = []
        user_channels = {}

        for user in case_data["users"]:
            # Calculate required SNR based on app, distance, and CSI
            required_snr_dB = self.calculate_required_snr(user["app"], user["coord"], user["csi"])

            # Calculate actual received SNR
            received_snr_dB, h_d, H, h_1 = self.calculate_received_snr(user, bs_power_linear, noise_power_linear)

            # Calculate delta SNR
            delta_snr = received_snr_dB - required_snr_dB
            delta_snrs.append(delta_snr)

            # Store channel information and calculated required SNR
            user_channels[user["id"]] = {
                "h_d": h_d, "H": H, "h_1": h_1,
                "required_snr_dB": required_snr_dB
            }
            user.pop("app", None)  # remove app field
            user["required_snr_dB"] = float(required_snr_dB)
            user["achieved_snr_dB"] = float(received_snr_dB)
            user["delta_snr_dB"] = float(delta_snr)
            # If user is not satisfied (delta SNR < 0), select for optimization
            if delta_snr < 0:
                selected_users.append(user["id"])


        return selected_users, delta_snrs, user_channels

    def calculate_qos(self, required_snr: float, achieved_snr: float) -> float:
        """
        Calculate QoS as the difference between achieved and required SNR.
        Args:
            required_snr (float): The target SNR requirement.
            achieved_snr (float): The actual achieved SNR.
        Returns:
            float: QoS value (positive = good, negative = poor).
        """
        return achieved_snr - required_snr

    def run_algorithm_comparison(self, user_channels: Dict, selected_users: List[int],
                               bs_power_linear: float, noise_power_linear: float) -> str:
        """Run exhaustive search across all algorithms to find the best one based on average QoS."""
        if not selected_users:
            return "no_optimization_needed"

        algorithm_results = {}

        # Collect channel data for selected users
        h_d_list = [user_channels[uid]["h_d"] for uid in selected_users]
        H_list = [user_channels[uid]["H"] for uid in selected_users] # Note: H is BS-to-RIS, same for all users
        h_1_list = [user_channels[uid]["h_1"] for uid in selected_users]

        for algo_name in self.algorithm_names:
            per_user_qos = []

            try:
                # Run the multi-user algorithm
                algo_method = getattr(self.algorithms, algo_name)
                
                theta, sum_rates, per_user_snrs, iterations = algo_method(
                    h_d_list, H_list[0], h_1_list, self.sim_settings["ris_elements"],
                    bs_power_linear, max_iterations=self.sim_settings["iterations"],
                    noise_power=noise_power_linear
                )

                # Calculate QoS for each selected user based on the final per-user SNRs
                final_per_user_snrs = per_user_snrs[-1] if per_user_snrs else [-999.0] * len(selected_users)
                
                # Log final SNRs calculated
                print(f"📊 {algo_name} Final SNRs (dB): {np.round(final_per_user_snrs, 2)}")
                
                for i, user_id in enumerate(selected_users):
                    required_snr = user_channels[user_id]["required_snr_dB"]
                    achieved_snr = final_per_user_snrs[i]
                    qos = self.calculate_qos(required_snr, achieved_snr)
                    per_user_qos.append(qos)

            except Exception as e:
                print(f"Algorithm {algo_name} failed: {e}")
                # If algorithm fails, assign a large negative QoS penalty for all selected users
                per_user_qos = [-1000.0] * len(selected_users)

            # average QoS across selected users for this algorithm
            if per_user_qos:
                avg_qos = float(np.mean(per_user_qos))
            else:
                avg_qos = -np.inf  # shouldn't happen since selected_users not empty

            algorithm_results[algo_name] = avg_qos

        # Select the algorithm with highest average QoS
        best_algorithm = max(algorithm_results, key=algorithm_results.get)
        return best_algorithm

    def generate_training_example(self, case_data: Dict) -> Dict:
        """Generate a single training example."""
        # Step 1: Select users for optimization
        selected_users, delta_snrs, user_channels = self.select_users_for_optimization(case_data)

        # Step 2: Find best algorithm
        bs_power_linear = 10 ** ((self.sim_settings["default_bs_power_dBm"] - 30) / 10)
        noise_power_linear = 10 ** ((self.sim_settings["noise_power_dBm"] - 30) / 10)
        best_algorithm = self.run_algorithm_comparison(user_channels, selected_users, bs_power_linear, noise_power_linear)

        # Step 3: Create feature vector from case data
        features = self.extract_features(case_data)

        # Step 4: Create training example
        # Essential sim settings only (remove static parameters)
        essential_sim_settings = {
            "ris_elements": self.sim_settings["ris_elements"],
            "base_station_power_dBm": self.sim_settings["default_bs_power_dBm"],
            "noise_power_dBm": self.sim_settings["noise_power_dBm"],
            "bs_coord": self.sim_settings["bs_coord"],
            "ris_coord": self.sim_settings["ris_coord"]
        }

        example = {
            "input": {
                "case_data": case_data,
                "sim_settings": essential_sim_settings
            },
            "output": {
                "selected_users": selected_users,
                "best_algorithm": best_algorithm
            }
        }

        return example

    def extract_features(self, case_data: Dict) -> List[float]:
      """Extract numerical features from case data for ML training."""
      features = []

      # Global features
      features.append(case_data["num_users"])

      # Per-user features (pad/truncate to fixed size for consistency)
      max_users = 5  # Assume maximum 5 users for fixed feature size

      for i in range(max_users):
          if i < len(case_data["users"]):
              user = case_data["users"][i]
              # Coordinate features
              features.extend(user["coord"])
              # CSI features (encoded)
              features.append(1.0 if user["csi"]["los"] else 0.0)
              features.append(1.0 if user["csi"]["fading"] == "Rician" else 0.0)
              features.append(user["csi"].get("K_factor_dB", 0))
              # --- NEW: use SNR metrics instead of app ---
              features.append(user["required_snr_dB"])
              features.append(user["achieved_snr_dB"])
              features.append(user["delta_snr_dB"])
          else:
              # Pad with zeros for missing users
              features.extend([0.0] * 9)  # 3 coords + 3 CSI + 3 SNR metrics

      return features

    def generate_dataset(self, num_examples: int = 1000) -> List[Dict]:
        """Generate the complete dataset."""
        print(f"Generating {num_examples} training examples...")

        dataset = []

        for i in tqdm(range(num_examples)):
            # Generate random case data
            case_data = self.generate_random_case()

            # Generate training example
            example = self.generate_training_example(case_data)
            dataset.append(example)

        return dataset

    def generate_random_case(self) -> Dict:
        """Generate random case data for training."""
        num_users = np.random.randint(3, 6)  # 3-5 users
        users = []

        applications = FRAMEWORK_CONFIG.get("snr_calculation", {}).get("applications", {})
        app_types = list(applications.keys())

        for user_id in range(1, num_users + 1):
            app = np.random.choice(app_types)

            # Random position around RIS
            x = np.random.uniform(30, 200)
            y = np.random.uniform(-60, 60)
            z = 1.5

            # CSI characteristics - simplified as requested
            los = np.random.choice([True, False], p=[0.6, 0.4])  # 60% LoS probability
            fading = "Rician" if los and np.random.random() > 0.3 else np.random.choice(["Rician", "Rayleigh"], p=[0.4, 0.6])

            csi = {
                "los": los,
                "fading": fading
            }

            # Add K-factor for Rician fading
            if fading == "Rician":
                # LoS typically has higher K-factor
                if los:
                    csi["K_factor_dB"] = np.random.uniform(5, 12)
                else:
                    csi["K_factor_dB"] = np.random.uniform(0, 6)

            user = {
                "id": user_id,
                "coord": (x, y, z),
                "app": app,
                "csi": csi
            }

            users.append(user)

        case_data = {
            "num_users": num_users,
            "users": users
        }

        return case_data

    def save_dataset(self, dataset: List[Dict], filename: str):
      """Save dataset to JSON file."""
      serializable_dataset = to_serializable(dataset)
      with open(filename, 'w') as f:
          json.dump(serializable_dataset, f, indent=2)
      print(f"Dataset saved to {filename}")
    def generate_analysis_dataset(self, num_examples: int = 5) -> List[Dict]:
      """Generate dataset with per-algorithm SNRs for visualization/analysis."""
      print(f"Generating {num_examples} analysis examples with full SNR tracking...")

      dataset = []
      bs_power_linear = 10 ** ((self.sim_settings["default_bs_power_dBm"] - 30) / 10)
      noise_power_linear = 10 ** ((self.sim_settings["noise_power_dBm"] - 30) / 10)

      for i in range(num_examples):
          case_data = self.generate_random_case()
          selected_users, delta_snrs, user_channels = self.select_users_for_optimization(case_data)

          # Run all algorithms, collect achieved SNRs per user
          per_algo_results = {}

          # Collect channel data for selected users
          h_d_list = [user_channels[uid]["h_d"] for uid in selected_users]
          H_list = [user_channels[uid]["H"] for uid in selected_users] # Note: H is BS-to-RIS, same for all users
          h_1_list = [user_channels[uid]["h_1"] for uid in selected_users]


          for algo_name in self.algorithm_names + ["random"]:
              per_user_results = {}

              if algo_name == "random":
                  # Random phases baseline
                  theta = np.random.uniform(0, 2*np.pi, self.sim_settings["ris_elements"])
                  e = np.exp(1j * theta)

                  # Calculate received power and SNR for each user with random phases
                  for user_id in selected_users:
                      channels = user_channels[user_id]
                      h_d, H, h_1 = channels["h_d"], channels["H"], channels["h_1"]
                      required_snr = channels["required_snr_dB"]
                      h_r = self.algorithms.compute_cascaded_channel(H, h_1)
                      power = self.algorithms.compute_received_power(h_d, h_r, e, bs_power_linear)
                      achieved_snr = self.algorithms.compute_snr(power, noise_power_linear)
                      per_user_results[user_id] = {
                          "required_snr": required_snr,
                          "achieved_snr": achieved_snr
                      }
              else:
                  try:
                      algo_method = getattr(self.algorithms, algo_name)
                      theta, sum_rates, per_user_snrs, iters = algo_method(
                          h_d_list, H_list[0], h_1_list, self.sim_settings["ris_elements"],
                          bs_power_linear, max_iterations=self.sim_settings["iterations"],
                          noise_power=noise_power_linear
                      )
                      final_per_user_snrs = per_user_snrs[-1] if per_user_snrs else [-999.0] * len(selected_users)
                      for i, user_id in enumerate(selected_users):
                          required_snr = user_channels[user_id]["required_snr_dB"]
                          achieved_snr = final_per_user_snrs[i]
                          per_user_results[user_id] = {
                              "required_snr": required_snr,
                              "achieved_snr": achieved_snr
                          }
                  except Exception as e:
                      print(f"Algorithm {algo_name} failed: {e}")
                      for user_id in selected_users:
                          required_snr = user_channels[user_id]["required_snr_dB"]
                          per_user_results[user_id] = {
                              "required_snr": required_snr,
                              "achieved_snr": -999.0 # Indicate failure
                          }


              per_algo_results[algo_name] = per_user_results

          dataset.append({
              "case_data": case_data,
              "selected_users": selected_users,
              "per_algorithm_results": per_algo_results
          })

      return dataset
    def save_analysis_dataset(self, dataset: List[Dict], filename: str):
      """Save analysis dataset with per-algorithm SNRs."""
      serializable_dataset = to_serializable(dataset)
      with open(filename, 'w') as f:
          json.dump(serializable_dataset, f, indent=2)
      print(f"Analysis dataset saved to {filename}")

"""#**Generate Dataset Here**

### To Add: A new file that stores exact csi, and remaining conditions for each example

###**Recommended Training Set Size --- 15,000**
"""

def main():
    """Main function to generate the dataset."""
    print("=== RIS User Selection and Algorithm Choice Dataset Generation ===")

    # Initialize generator
    generator = RISDatasetGenerator()

    # Generate dataset
    dataset = generator.generate_dataset(num_examples=5000)

    # Save dataset
    generator.save_dataset(dataset, 'ris_user_algorithm_dataset.json')

    # Print statistics
    print(f"\n=== Dataset Statistics ===")
    print(f"Total examples: {len(dataset)}")

    # Algorithm distribution
    algorithm_counts = {}
    for example in dataset:
        algo = example["output"]["best_algorithm"]
        algorithm_counts[algo] = algorithm_counts.get(algo, 0) + 1

    print("Algorithm selection distribution:")
    for algo, count in algorithm_counts.items():
        print(f"  {algo}: {count} ({count/len(dataset)*100:.1f}%)")

    # User selection statistics
    selected_users_data = []
    satisfaction_rates = []

    for example in dataset:
        num_selected = len(example["output"]["selected_users"])
        total_users = example["input"]["case_data"]["num_users"]
        selected_users_data.append(num_selected)
        satisfaction_rates.append((total_users - num_selected) / total_users)

    print(f"\nUser selection statistics:")
    print(f"  Average users selected: {np.mean(selected_users_data):.2f}")
    print(f"  Max users selected: {np.max(selected_users_data)}")
    print(f"  Min users selected: {np.min(selected_users_data)}")

    # Satisfaction rate
    print(f"\nSatisfaction rate statistics:")
    print(f"  Average satisfaction rate: {np.mean(satisfaction_rates):.2f}")
    print(f"  Cases needing optimization: {sum(1 for ex in dataset if ex['output']['best_algorithm'] != 'no_optimization_needed')}")

    # --- NEW: SNR statistics instead of application distribution ---
    required_snr_values = []
    achieved_snr_values = []
    delta_snr_values = []

    for example in dataset:
        for user in example["input"]["case_data"]["users"]:
            required_snr_values.append(user["required_snr_dB"])
            achieved_snr_values.append(user["achieved_snr_dB"])
            delta_snr_values.append(user["delta_snr_dB"])

    print(f"\nSNR statistics across all users:")
    print(f"  Avg required SNR: {np.mean(required_snr_values):.2f} dB")
    print(f"  Avg achieved SNR: {np.mean(achieved_snr_values):.2f} dB")
    print(f"  Avg delta SNR: {np.mean(delta_snr_values):.2f} dB")
    print(f"  % of users satisfied: {100 * np.mean([1 if d >= 0 else 0 for d in delta_snr_values]):.1f}%")

    print("\nDataset generation completed successfully!")


if __name__ == "__main__":
    main()

"""###***Please Confirm Via Visualization that dataset is correct***

####Add another document along side the dataset that stores average SNR, delta SNR, random SNR and single algorithm SNR so we can verify that the system is working as expected

**If not then we work on finding the issue**
"""